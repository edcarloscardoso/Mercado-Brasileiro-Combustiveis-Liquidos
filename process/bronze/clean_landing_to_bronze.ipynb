{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "437bafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, when\n",
    "import unicodedata\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26d353bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LandingToBronze\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2232c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "landing_path = \"../../datalake/landing\"\n",
    "bronze_path = \"../../datalake/bronze\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a76274d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_historico = [\n",
    "    \"ano\", \"mes\", \"agente_regulado\", \"codigo_produto\", \"nome_produto\",\n",
    "    \"regiao_origem\", \"uf_origem\", \"regiao_destino\", \"uf_destino\",\n",
    "    \"destino_consumo\", \"volume_vendido\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a79b832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class datanormalizer:\n",
    "    @staticmethod\n",
    "    def padronizar_nomes(df):\n",
    "        coluna_limpa = [\n",
    "            unicodedata.normalize('NFKD', c.lower().strip())\n",
    "            .encode('ascii', 'ignore')\n",
    "            .decode('utf-8')\n",
    "            .replace(' ', '_')\n",
    "            for c in df.columns\n",
    "        ]\n",
    "        for old, new in zip(df.columns, coluna_limpa):\n",
    "            df = df.withColumnRenamed(old, new)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a7f90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_cleaner:\n",
    "    @staticmethod\n",
    "    def limpeza_basica(df):\n",
    "        df = df.dropDuplicates()\n",
    "        df = df.na.drop(how='all')\n",
    "        for col_name in df.columns:\n",
    "            if \"data\" in col_name or \"mes\" in col_name:\n",
    "                df = df.withColumn(col_name,\n",
    "                    when(col(col_name).rlike(r\"^\\d{4}-\\d{2}-\\d{2}$\"), to_date(col(col_name), \"yyyy-MM-dd\"))\n",
    "                    .otherwise(None))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eabbb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/17 17:56:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/06/17 17:56:34 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/06/17 17:56:36 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for arquivo in os.listdir(landing_path):\n",
    "    if arquivo.endswith(\".csv\"):\n",
    "        caminho_arquivo = os.path.join(landing_path, arquivo)\n",
    "        nome_base = os.path.splitext(arquivo)[0]\n",
    "        destino = os.path.join(bronze_path, nome_base)\n",
    "\n",
    "        if nome_base == \"Liquidos_Vendas_Historico_2007_a_2017\":\n",
    "            df = spark.read.option(\"header\", False)\\\n",
    "                           .option(\"encoding\", \"iso-8859-1\")\\\n",
    "                           .option(\"delimiter\", \";\")\\\n",
    "                           .csv(caminho_arquivo)\n",
    "            df = df.toDF(*colunas_historico)\n",
    "        else:\n",
    "            df = spark.read.option(\"header\", True)\\\n",
    "                           .option(\"encoding\", \"iso-8859-1\")\\\n",
    "                           .option(\"delimiter\", \";\")\\\n",
    "                           .csv(caminho_arquivo)\n",
    "        \n",
    "        df = datanormalizer.padronizar_nomes(df)\n",
    "        df = data_cleaner.limpeza_basica(df)\n",
    "\n",
    "        df.write.mode(\"overwrite\").parquet(destino)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
