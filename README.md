ğŸ›¢ï¸ Mercado Brasileiro de CombustÃ­veis LÃ­quidos  
ğŸ“Š Fonte dos Dados  
Os dados utilizados neste projeto foram extraÃ­dos do Painel DinÃ¢mico do Mercado Brasileiro de CombustÃ­veis LÃ­quidos, disponibilizado pela AgÃªncia Nacional do PetrÃ³leo, GÃ¡s Natural e BiocombustÃ­veis (ANP).  

O painel fornece informaÃ§Ãµes atualizadas periodicamente (dias 1Âº e 20 de cada mÃªs) sobre a comercializaÃ§Ã£o, distribuiÃ§Ã£o e localizaÃ§Ã£o geogrÃ¡fica de agentes do setor de combustÃ­veis lÃ­quidos no Brasil. A base de dados Ã© pÃºblica e gratuita.  

ğŸ¯ Objetivo  
Este projeto tem como objetivo extrair, padronizar, limpar, transformar e analisar os dados da ANP utilizando PySpark, com foco em tratar grandes volumes de dados e gerar insights sobre o consumo e a distribuiÃ§Ã£o de combustÃ­veis lÃ­quidos no Brasil.  

ğŸ—‚ï¸ Estrutura do Projeto  
ğŸ“ Datalake (Armazenamento em camadas)  
landing/ â€” Dados brutos originais (.csv) extraÃ­dos da fonte oficial.  

bronze/ â€” Dados convertidos para Parquet com padronizaÃ§Ã£o inicial.  

silver/ â€” Dados limpos, tratados e normalizados.  

gold/ â€” Dados agregados e prontos para anÃ¡lise.  

âš™ï¸ Workflow (Pipeline de Processamento)  
OrganizaÃ§Ã£o modular dos scripts em PySpark, por etapas do processo:  

ExtraÃ§Ã£o dos arquivos  

PadronizaÃ§Ã£o de colunas   

Limpeza e tratamento de dados  

AnÃ¡lises e geraÃ§Ã£o de indicadores  

ğŸ§ª Passo a Passo TÃ©cnico  
1. ğŸ§¾ ExtraÃ§Ã£o e Armazenamento Inicial (Landing Zone)  
Bibliotecas utilizadas:

requests â€” download automÃ¡tico dos arquivos.

zipfile â€” descompactaÃ§Ã£o dos arquivos .zip.

os â€” manipulaÃ§Ã£o de diretÃ³rios locais.

urllib3 â€” desativaÃ§Ã£o de avisos SSL (problemas recorrentes no site da ANP).

Objetivo:
Baixar e extrair os dados originais, armazenando na pasta landing/.

2. ğŸ“¦ PadronizaÃ§Ã£o e ConversÃ£o (Bronze Zone)
Bibliotecas utilizadas:

pyspark.sql.SparkSession â€” sessÃ£o Spark para processamentos distribuÃ­dos.

unicodedata â€” remoÃ§Ã£o de acentos e caracteres especiais.

os â€” iteraÃ§Ã£o sobre arquivos locais.

Objetivo:
Padronizar os nomes das colunas e converter os arquivos CSV em Parquet, otimizando o desempenho para big data.

3. ğŸ§¹ Limpeza e NormalizaÃ§Ã£o (Silver Zone)
Bibliotecas utilizadas:

pyspark.sql.functions (to_date, when, col, etc.) â€” tratamento de nulos, datas e duplicidades.

pyspark.sql.SparkSession â€” continuidade do processamento.

Objetivo:
Remover linhas invÃ¡lidas ou duplicadas e ajustar os tipos de dados, garantindo a integridade e consistÃªncia da base.

4. ğŸ“ˆ AnÃ¡lise e Indicadores (Gold Zone)
Bibliotecas utilizadas:

pyspark.sql.functions (regexp_replace, sum, col, etc.) â€” transformaÃ§Ãµes e agregaÃ§Ãµes.

pyspark.sql.SparkSession â€” execuÃ§Ã£o distribuÃ­da.

Objetivo:
Gerar insights a partir da base tratada, como:

TendÃªncia de consumo por tipo de produto e por ano;

Ranking de estados e municÃ­pios com maior volume de comercializaÃ§Ã£o;

IdentificaÃ§Ã£o de polos regionais de distribuiÃ§Ã£o e consumo;

Ranking dos produtos mais consumidos ao longo do tempo.


1. ğŸ“‚ Estrutura de DiretÃ³rios
bash
Copy
Edit
workflow/  
â”‚  
â”œâ”€â”€ ingestion/  
â”‚   â””â”€â”€ bronze_loader.py  
â”‚  
â”œâ”€â”€ process/  
â”‚   â”œâ”€â”€ silver_cleaner.py  
â”‚   â””â”€â”€ gold_transformer.py  
â”‚  
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ cleaner.py            # FunÃ§Ãµes reutilizÃ¡veis (como limpar_colunas)
â”‚   â””â”€â”€ paths.py              # Constantes com caminhos
